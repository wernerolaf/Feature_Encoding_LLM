{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"gpt2\"  # or \"meta-llama/Llama-3.1-8B-Instruct\", etc.\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "# Many causal LMs donâ€™t have a pad token; set it to eos if needed:\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)\n",
    "model.eval()\n",
    "\n",
    "def final_token_embeddings_causal(texts, layer=-1, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Returns a tensor of shape [batch, hidden] with the contextual embedding\n",
    "    of the final (last non-pad) token in each input sequence.\n",
    "    \"\"\"\n",
    "    enc = tok(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    output_hidden_states=True)\n",
    "    # hidden_states is a tuple: (embeddings, layer1, ..., last_layer)\n",
    "    last_hid = out.hidden_states[layer]  # -1 = final layer\n",
    "    # index of last non-pad token in each row\n",
    "    last_idx = attention_mask.sum(dim=1) - 1  # [batch]\n",
    "    batch_idx = torch.arange(last_hid.size(0), device=device)\n",
    "    final_vecs = last_hid[batch_idx, last_idx]  # [batch, hidden]\n",
    "    return final_vecs\n",
    "\n",
    "# Example\n",
    "texts = [\"The nurse said their patient felt better.\", \"The doctor finished the rounds.\"]\n",
    "vecs = final_token_embeddings_causal(texts)   # [2, hidden]\n",
    "print(vecs.shape)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
